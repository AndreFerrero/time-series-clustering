{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run 1-setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's standardize/normalize the numerical features of the dataframe\n",
    "for df in dataframes.values():\n",
    "    df[numerics] = MinMaxScaler().fit_transform(df[numerics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['modena']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Series Kmeans requires a 3D array, so let's create it from all the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provinces = dataframes.keys()\n",
    "\n",
    "# Prepare input data\n",
    "X = []\n",
    "\n",
    "for province in provinces:\n",
    "    group_data = dataframes[province][numerics].values\n",
    "    \n",
    "    # Reshape to (n_samples, n_timestamps, n_features)\n",
    "    group_data = np.expand_dims(group_data, axis=0)\n",
    "    X.append(group_data)\n",
    "\n",
    "# Stack the list of arrays to create a 3D array\n",
    "X = np.vstack(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to choose the optimal number of clusters. To do this, we'll run kmeans different times for different cluster sizes.\n",
    "Each time we'll save the total inertia, which is a measure of goodness of fit for clustering. It represents the sum of squared distances of all observations from the respective cluster center. The lower the inertia, the more the clusters are concentrated around the centroids. For time series data, the best choice is to use Dinamic Time Warping (dtw) as metric for clustering, since Euclidean distance is not invariant to time shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "K = list(range(2, 9))\n",
    "\n",
    "for k in K:\n",
    "    km = TimeSeriesKMeans(n_clusters=k, n_init=5, metric='dtw', random_state=0)\n",
    "    \n",
    "    km = km.fit(X)\n",
    "    \n",
    "    inertia.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering both preprocessing steps (standardization and min-max scaling) by looking at the 2 figures for the elbow method, we can deduce that it's unlikely KMeans did a good job at clustering, as there is not a clear difference for the various Ks. Ideally, we should see a number of clusters that has a clear drop with respect to the previous one, while in this case the direction is constantly decreasing almost at the same rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(K, inertia)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Total sum of squares')\n",
    "plt.title('Elbow method')\n",
    "\n",
    "x_ticks = np.linspace(min(K), max(K), 7)\n",
    "\n",
    "# Set the ticks on the x-axis\n",
    "plt.xticks(x_ticks)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = TimeSeriesKMeans(n_clusters=3, n_init=5, metric='dtw', random_state=0).fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, cluster in zip(dataframes.values(), clusters):\n",
    "    df['cluster'] = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['bologna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat(dataframes.values(), ignore_index=True)\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style of seaborn for better aesthetics\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "for var in pollutants:\n",
    "    \n",
    "    # Plot the time series for each province\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(x='date', y=var, hue='cluster', data=full_df)\n",
    "\n",
    "    plt.title('Meteorological Information by cluster')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(var)\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
