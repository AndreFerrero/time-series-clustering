{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the folder containing your CSV files\n",
    "folder_path = R\"C:\\Users\\andre\\OneDrive - Alma Mater Studiorum UniversitÃ  di Bologna\\University\\UniBo\\Machine Learning\\PR2.20\\data\"\n",
    "\n",
    "# List all files in the folder\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith(\".csv\") if file != 'InfoComune.csv']\n",
    "\n",
    "# Create an empty dictionary to store DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Iterate through each CSV file\n",
    "for file in csv_files:\n",
    "    # Extract the file name\n",
    "    df_name = os.path.splitext(file)[0]\n",
    "    \n",
    "    # Create the DataFrame and store it in the dictionary\n",
    "    dataframes[df_name] = pd.read_csv(os.path.join(folder_path, file), header=0, skiprows = [1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store dictionary items in specific variables to make it easier to loop through them\n",
    "datasets = dataframes.values()\n",
    "provinces = dataframes.keys()\n",
    "data_prov_pairs = dataframes.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in datasets:\n",
    "    # change missing values to the proper format\n",
    "    df.replace('---', np.nan, inplace = True)\n",
    "    # ensure a unique format\n",
    "    df = df.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['bologna'].columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['bologna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to keep the average value only\n",
    "pollutants = ['CO', 'NH3', 'NMVOC', 'NO2', 'NO', 'O3', 'PANS', 'PM10', 'PM2.5', 'SO2']\n",
    "\n",
    "# metereological information\n",
    "met = ['TG', 'TN', 'TX', 'HU', 'PP', 'QQ', 'RR']\n",
    "met_pos = range(6, 13)\n",
    "# date values\n",
    "date = ['YYYY', 'MM', 'DD']\n",
    "date_pos = list(range(3))\n",
    "\n",
    "# rename the columns for date and metereological information\n",
    "for df in datasets:\n",
    "    old_date = df.columns[date_pos]\n",
    "    old_met = df.columns[met_pos]\n",
    "    \n",
    "    df.rename(columns=dict(zip(old_date, date)), inplace=True)\n",
    "    df.rename(columns=dict(zip(old_met, met)), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['bologna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = date + met + pollutants\n",
    "\n",
    "for province, df in data_prov_pairs:\n",
    "    # Keep only selected columns\n",
    "    dataframes[province] = df[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['bologna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a date variable for all the dataframes\n",
    "for province, df in data_prov_pairs:\n",
    "    # Combine 'YYYY', 'MM', 'DD' columns into a new 'date' column\n",
    "    df['date'] = pd.to_datetime(df[['YYYY', 'MM', 'DD']].astype(str).agg('-'.join, axis=1), format='%Y-%m-%d')\n",
    "    \n",
    "    # Remove 'YYYY', 'MM', 'DD' columns\n",
    "    df.drop(['YYYY', 'MM', 'DD'], axis=1, inplace=True)\n",
    "    \n",
    "    # Reorder columns with 'date' as the first column\n",
    "    dataframes[province] = df[['date'] + [col for col in df.columns if col != 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['bologna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = met + pollutants\n",
    "\n",
    "for province, df in data_prov_pairs:\n",
    "    # first convert to numeric the columns in met and pollutants, since they are strings\n",
    "    df[numerics] = df[numerics].apply(pd.to_numeric, errors = 'coerce')\n",
    "    # round to the second decimal number for better visualization\n",
    "    df[numerics] = df[numerics].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['bologna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to filter the series so that we don't have missing values\n",
    "# We'll start from 2018-01-01 and move until 2020-12-28\n",
    "\n",
    "for province, df in data_prov_pairs:\n",
    "    df = df[(df['date'] >= pd.to_datetime('2018-01-01')) & (df['date'] <= pd.to_datetime('2020-12-28'))]\n",
    "\n",
    "    dataframes[province] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['bologna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we still have 1 missing value\n",
    "dataframes['bologna'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the missing value is the 29th of February\n",
    "dataframes['bologna'][dataframes['bologna'].isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the remaining missing value is not missing at the beginning or the end of the series, it was decided to impute the value with the median, since it's robust to extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "for province, df in data_prov_pairs:\n",
    "    df[numerics] = imputer.fit_transform(df[numerics])\n",
    "    \n",
    "    dataframes[province] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['bologna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['bologna'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following we apply smoothing to the series, in order to reduce noise and the impact of outliers on the dataset.\n",
    "\n",
    "The smoothing function is the Savitzky-Golay function, which applies polynomial smooting on the time span indicated by the window parameter.\n",
    "\n",
    "The advantage of this function compared to other smoothing methods like moving average is that it doesn't introduce missing values in the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "\n",
    "def smooth(window, window_pp, poly_pp, poly = 2):\n",
    "    \n",
    "    for province, df in data_prov_pairs:\n",
    "        \n",
    "        for column in df[numerics].columns:\n",
    "            # Extract numerical values from the DataFrame\n",
    "            values = df[column].values\n",
    "            \n",
    "            # Apply Savitzky-Golay filter to the numerical values\n",
    "            # for PP we need higher smooting due to outliers\n",
    "            if column == 'PP':\n",
    "                window = window_pp\n",
    "                poly = poly_pp\n",
    "                \n",
    "            smoothed_values = savgol_filter(values, window, poly)\n",
    "            # Update the DataFrame with the smoothed values\n",
    "            df[column] = smoothed_values\n",
    "            \n",
    "        dataframes[province] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth(window = 10, window_pp = 25, poly_pp = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
